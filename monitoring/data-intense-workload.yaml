apiVersion: v1
kind: ConfigMap
metadata:
  name: workload-simulator-config
  namespace: data-locality-scheduler
data:
  simulate.sh: |
    #!/bin/bash
    set -e

    echo "Starting data locality test workload..."

    if ! mc ls minio/test-bucket/large-test-file.bin &>/dev/null; then
      echo "Creating 50MB test file..."
      dd if=/dev/urandom of=/tmp/large-test-file.bin bs=1M count=50
      mc cp /tmp/large-test-file.bin minio/test-bucket/
      echo "Test file created and uploaded to main MinIO"
    fi

    if ! mc ls minio/test-bucket/medium-test-file.bin &>/dev/null; then
      echo "Creating 20MB test file..."
      dd if=/dev/urandom of=/tmp/medium-test-file.bin bs=1M count=20
      mc cp /tmp/medium-test-file.bin minio/test-bucket/
      echo "Medium test file created and uploaded"
    fi

    # region-specific test data
    if ! mc ls region1/region1-bucket/region1-data.bin &>/dev/null; then
      echo "Creating 30MB region1 test file..."
      dd if=/dev/urandom of=/tmp/region1-data.bin bs=1M count=30
      mc cp /tmp/region1-data.bin region1/region1-bucket/
      echo "Region1 test file created and uploaded"
    fi

    if ! mc ls region2/region2-bucket/region2-data.bin &>/dev/null; then
      echo "Creating 30MB region2 test file..."
      dd if=/dev/urandom of=/tmp/region2-data.bin bs=1M count=30
      mc cp /tmp/region2-data.bin region2/region2-bucket/
      echo "Region2 test file created and uploaded"
    fi

    echo "All test data created successfully"

    sleep 30
---
apiVersion: batch/v1
kind: Job
metadata:
  name: data-prep-job
  namespace: data-locality-scheduler
spec:
  template:
    metadata:
      labels:
        app: data-locality-test
        component: data-prep
    spec:
      restartPolicy: OnFailure
      containers:
      - name: data-creator
        image: minio/mc
        command: [ "/bin/bash", "-c" ]
        args:
        - |
          mc config host add minio http://minio:9000 minioadmin minioadmin
          mc config host add region1 http://minio-edge-region1:9000 minioadmin minioadmin
          mc config host add region2 http://minio-edge-region2:9000 minioadmin minioadmin

          # Run the simulation script
          cat > /tmp/simulate.sh << 'EOF'
          #!/bin/bash
          set -e

          echo "Starting data locality test workload..."

          if ! mc ls minio/test-bucket/large-test-file.bin &>/dev/null; then
            echo "Creating 50MB test file..."
            dd if=/dev/urandom of=/tmp/large-test-file.bin bs=1M count=50
            mc cp /tmp/large-test-file.bin minio/test-bucket/
            echo "Test file created and uploaded to main MinIO"
          fi

          if ! mc ls minio/test-bucket/medium-test-file.bin &>/dev/null; then
            echo "Creating 20MB test file..."
            dd if=/dev/urandom of=/tmp/medium-test-file.bin bs=1M count=20
            mc cp /tmp/medium-test-file.bin minio/test-bucket/
            echo "Medium test file created and uploaded"
          fi

          # region-specific test data
          if ! mc ls region1/region1-bucket/region1-data.bin &>/dev/null; then
            echo "Creating 30MB region1 test file..."
            dd if=/dev/urandom of=/tmp/region1-data.bin bs=1M count=30
            mc cp /tmp/region1-data.bin region1/region1-bucket/
            echo "Region1 test file created and uploaded"
          fi

          if ! mc ls region2/region2-bucket/region2-data.bin &>/dev/null; then
            echo "Creating 30MB region2 test file..."
            dd if=/dev/urandom of=/tmp/region2-data.bin bs=1M count=30
            mc cp /tmp/region2-data.bin region2/region2-bucket/
            echo "Region2 test file created and uploaded"
          fi

          # edge-data bucket if it doesn't exist
          if ! mc ls region1/edge-data &>/dev/null; then
            echo "Creating edge-data bucket..."
            mc mb region1/edge-data
            echo "Edge data bucket created"
          fi

          # sample sensor data
          if ! mc ls region1/edge-data/sensor-data.json &>/dev/null; then
            echo "Creating sensor data file..."
            echo '{"timestamp":"2025-04-17T12:00:00Z","readings":[]}' > /tmp/sensor-data.json
            dd if=/dev/urandom bs=1M count=10 >> /tmp/sensor-data.json
            mc cp /tmp/sensor-data.json region1/edge-data/
            echo "Sensor data created and uploaded"
          fi

          echo "All test data created successfully"
          EOF

          chmod +x /tmp/simulate.sh
          /tmp/simulate.sh
---
apiVersion: batch/v1
kind: Job
metadata:
  name: local-data-processor
  namespace: data-locality-scheduler
spec:
  template:
    metadata:
      annotations:
        data.scheduler.thesis/input-1: "test-bucket/medium-test-file.bin,20971520,10,8,binary"
        data.scheduler.thesis/output-1: "test-bucket/local-processed.json,5242880,0,6,json"
        scheduler.thesis/data-intensive: "true"
      labels:
        app: data-locality-test
        component: local-processor
    spec:
      schedulerName: data-locality-scheduler
      restartPolicy: OnFailure
      containers:
      - name: processor
        image: python:3.9-slim
        command: [ "/bin/bash", "-c" ]
        args:
        - |
          echo "Starting local data processor..."
          echo "NODE_NAME: $NODE_NAME"
          echo "POD_NAME: $POD_NAME"
          echo "Expected to process data from test-bucket/medium-test-file.bin"

          # we simulate processing for 30 seconds
          sleep 30

          echo "Processing complete"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "400m"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: cross-region-processor
  namespace: data-locality-scheduler
spec:
  template:
    metadata:
      annotations:
        data.scheduler.thesis/input-1: "region1-bucket/region1-data.bin,31457280,15,7,binary"
        data.scheduler.thesis/output-1: "region2-bucket/cross-region-result.json,10485760,0,7,json"
        scheduler.thesis/data-intensive: "true"
        scheduler.thesis/prefer-region: "region-2"
      labels:
        app: data-locality-test
        component: cross-region-processor
    spec:
      schedulerName: data-locality-scheduler
      restartPolicy: OnFailure
      containers:
      - name: processor
        image: python:3.9-slim
        command: [ "/bin/bash", "-c" ]
        args:
        - |
          echo "Starting cross-region data processor..."
          echo "NODE_NAME: $NODE_NAME"
          echo "POD_NAME: $POD_NAME"
          echo "Expected to process data from region1-bucket/region1-data.bin"
          echo "Expected to output to region2-bucket/cross-region-result.json"

          # Simulate processing for 45 seconds
          sleep 45

          echo "Processing complete"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "400m"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: edge-cloud-processor
  namespace: data-locality-scheduler
spec:
  template:
    metadata:
      annotations:
        data.scheduler.thesis/input-1: "edge-data/sensor-data.json,10485760,5,8,json"
        data.scheduler.thesis/output-1: "test-bucket/cloud-result.json,5242880,0,7,json"
        scheduler.thesis/prefer-cloud: "true"
        scheduler.thesis/compute-intensive: "true"
      labels:
        app: data-locality-test
        component: edge-cloud-processor
    spec:
      schedulerName: data-locality-scheduler
      restartPolicy: OnFailure
      containers:
      - name: processor
        image: python:3.9-slim
        command: [ "/bin/bash", "-c" ]
        args:
        - |
          echo "Starting edge-to-cloud data processor..."
          echo "NODE_NAME: $NODE_NAME"
          echo "POD_NAME: $POD_NAME"
          echo "Expected to process data from edge-data/sensor-data.json"
          echo "Expected to output to test-bucket/cloud-result.json"

          # Simulate processing for 40 seconds
          sleep 40

          echo "Processing complete"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: same-region-transfer
  namespace: data-locality-scheduler
spec:
  template:
    metadata:
      annotations:
        data.scheduler.thesis/input-1: "test-bucket/large-test-file.bin,52428800,5,9,binary"
        data.scheduler.thesis/output-1: "region1-bucket/copied-large-file.bin,52428800,0,7,binary"
        scheduler.thesis/prefer-region: "region-1"
        scheduler.thesis/data-intensive: "true"
      labels:
        app: data-locality-test
        component: same-region-transfer
    spec:
      schedulerName: data-locality-scheduler
      restartPolicy: OnFailure
      containers:
      - name: transfer
        image: minio/mc
        command: [ "/bin/bash", "-c" ]
        args:
        - |
          echo "Starting same-region data transfer job..."
          echo "NODE_NAME: $NODE_NAME"
          echo "POD_NAME: $POD_NAME"

          # Configure MinIO client
          mc config host add minio http://minio:9000 minioadmin minioadmin
          mc config host add region1 http://minio-edge-region1:9000 minioadmin minioadmin

          # Check if input file exists
          if mc ls minio/test-bucket/large-test-file.bin; then
            echo "Input file exists, proceeding with transfer"
            
            # Download from source
            echo "Downloading from test-bucket..."
            mc cp minio/test-bucket/large-test-file.bin /tmp/large-test-file.bin
            
            # Simulate processing
            echo "Processing data..."
            sleep 20
            
            # Upload to destination
            echo "Uploading to region1-bucket..."
            mc cp /tmp/large-test-file.bin region1/region1-bucket/copied-large-file.bin
            
            echo "Transfer complete"
          else
            echo "Error: Input file not found"
            exit 1
          fi
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "400m"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: analytics-job
  namespace: data-locality-scheduler
spec:
  template:
    metadata:
      annotations:
        data.scheduler.thesis/input-1: "test-bucket/medium-test-file.bin,20971520,5,7,binary"
        data.scheduler.thesis/input-2: "region1-bucket/region1-data.bin,31457280,5,7,binary"
        data.scheduler.thesis/input-3: "region2-bucket/region2-data.bin,31457280,5,7,binary"
        data.scheduler.thesis/output-1: "test-bucket/analytics-result.json,1048576,0,6,json"
        scheduler.thesis/compute-intensive: "true"
        scheduler.thesis/prefer-cloud: "true"
      labels:
        app: data-locality-test
        component: analytics
    spec:
      schedulerName: data-locality-scheduler
      restartPolicy: OnFailure
      containers:
      - name: analytics
        image: python:3.9-slim
        command: [ "/bin/bash", "-c" ]
        args:
        - |
          echo "Starting analytics job..."
          echo "NODE_NAME: $NODE_NAME"
          echo "POD_NAME: $POD_NAME"
          echo "Processing multiple input files from different regions"

          # Simulate CPU-intensive processing
          echo "Running analytics calculations..."

          # Generate high CPU load for 30 seconds
          timeout 30s bash -c "while true; do echo 'CPU load' > /dev/null; done"

          echo "Analytics complete"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1"
