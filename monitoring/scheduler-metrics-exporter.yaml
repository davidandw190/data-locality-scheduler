apiVersion: apps/v1
kind: Deployment
metadata:
  name: scheduler-metrics-exporter
  namespace: monitoring
  labels:
    app: scheduler-metrics-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: scheduler-metrics-exporter
  template:
    metadata:
      labels:
        app: scheduler-metrics-exporter
    spec:
      containers:
      - name: metrics-exporter
        image: python:3.9-slim
        command: [ "/bin/bash" ]
        args: [ "-c", "pip install prometheus-client kubernetes && python /scripts/metrics.py" ]
        ports:
        - containerPort: 8000
          name: metrics
        volumeMounts:
        - name: scheduler-metrics-script
          mountPath: /scripts
        env:
        - name: KUBECONFIG
          value: /etc/kubernetes/kubeconfig
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
      volumes:
      - name: scheduler-metrics-script
        configMap:
          name: scheduler-metrics-script
---
apiVersion: v1
kind: Service
metadata:
  name: scheduler-metrics-exporter
  namespace: monitoring
  labels:
    app: scheduler-metrics-exporter
spec:
  selector:
    app: scheduler-metrics-exporter
  ports:
  - name: metrics
    port: 8000
    targetPort: 8000
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-metrics-script
  namespace: monitoring
data:
  metrics.py: |
    #!/usr/bin/env python3
    from prometheus_client import start_http_server, Counter, Gauge, Histogram, Summary
    import time
    import os
    import subprocess
    import json
    from kubernetes import client, config

    # Initialize Kubernetes client
    try:
        config.load_incluster_config()
    except:
        config.load_kube_config()

    k8s_client = client.CoreV1Api()

    # Create metrics
    scheduler_decision_time = Histogram(
        'scheduler_decision_time_seconds', 
        'Time taken for scheduler to make a decision',
        ['scheduler_name', 'pod_namespace', 'pod_type', 'result'],
        buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
    )

    scheduler_pod_placement = Counter(
        'scheduler_pod_placement_total', 
        'Pods placed by scheduler type',
        ['scheduler_name', 'node_type', 'region', 'zone', 'pod_namespace']
    )

    data_transfer_metrics = Gauge(
        'data_transfer_metrics',
        'Data transfer metrics between nodes',
        ['source_node', 'destination_node', 'metric_type']
    )

    node_data_locality_score = Gauge(
        'node_data_locality_score',
        'Data locality score for each node',
        ['node', 'region', 'zone', 'node_type']
    )

    edge_utilization = Gauge(
        'edge_utilization_ratio',
        'Ratio of pods scheduled on edge nodes',
        ['region', 'zone', 'scheduler_name']
    )

    pod_startup_latency = Histogram(
        'pod_startup_latency_seconds',
        'Time from scheduling to pod ready state',
        ['pod_namespace', 'node_type', 'scheduler_name'],
        buckets=[0.1, 0.5, 1, 2.5, 5, 10, 30, 60, 120, 300]
    )

    workflow_processing_time = Histogram(
        'workflow_processing_time_seconds',
        'End-to-end processing time for workflow components',
        ['workflow_name', 'component', 'scheduler_name', 'node_type'],
        buckets=[0.1, 0.5, 1, 2.5, 5, 10, 30, 60, 120, 300, 600]
    )

    # Start metrics server
    start_http_server(8000)
    print("Scheduler metrics server started on port 8000")

    def get_node_type(node_name):
        """Determine if node is edge or cloud based on labels or name"""
        try:
            node = k8s_client.read_node(node_name)
            labels = node.metadata.labels
            
            # Check for explicit label
            if 'node-capability/node-type' in labels:
                return labels['node-capability/node-type']
                
            # Check node name for edge indication
            if 'edge' in node_name.lower() or 'worker-node-1' in node_name or 'worker-node-2' in node_name:
                return 'edge'
            
            return 'cloud'
        except:
            # Default assumption based on node name patterns
            if 'edge' in node_name.lower() or 'worker-node-1' in node_name or 'worker-node-2' in node_name:
                return 'edge'
            return 'cloud'

    def get_node_region_zone(node_name):
        """Get region and zone for a node"""
        try:
            node = k8s_client.read_node(node_name)
            labels = node.metadata.labels
            
            region = labels.get('topology.kubernetes.io/region', 'unknown')
            zone = labels.get('topology.kubernetes.io/zone', 'unknown')
            
            return region, zone
        except:
            return 'unknown', 'unknown'

    def update_metrics():
        """Update all metrics based on cluster state"""
        try:
            # Get all nodes
            nodes = k8s_client.list_node().items
            node_info = {}
            
            edge_nodes = []
            cloud_nodes = []
            
            for node in nodes:
                node_name = node.metadata.name
                node_type = get_node_type(node_name)
                region, zone = get_node_region_zone(node_name)
                
                node_info[node_name] = {
                    'type': node_type,
                    'region': region,
                    'zone': zone
                }
                
                if node_type == 'edge':
                    edge_nodes.append(node_name)
                else:
                    cloud_nodes.append(node_name)
                
                # Simulate data locality scores
                score = 0
                if 'node-capability/storage-service' in node.metadata.labels:
                    score += 40
                    
                # Check for storage buckets
                bucket_count = 0
                for key in node.metadata.labels:
                    if key.startswith('node-capability/storage-bucket-'):
                        bucket_count += 1
                
                score += min(bucket_count * 5, 40)
                
                # Add base score
                if node_type == 'edge':
                    score += 15  # Edge nodes get a bonus for data locality
                
                # Record the score
                node_data_locality_score.labels(
                    node=node_name,
                    region=region,
                    zone=zone,
                    node_type=node_type
                ).set(score)
            
            # Get all pods with their scheduled nodes
            pods = k8s_client.list_pod_for_all_namespaces(watch=False).items
            
            edge_pods = {'data-locality-scheduler': 0, 'default-scheduler': 0, 'total': 0}
            cloud_pods = {'data-locality-scheduler': 0, 'default-scheduler': 0, 'total': 0}
            total_pods = {'data-locality-scheduler': 0, 'default-scheduler': 0, 'total': 0}
            
            for pod in pods:
                # Skip pods without node assignment
                if not pod.spec.node_name:
                    continue
                    
                node_name = pod.spec.node_name
                pod_namespace = pod.metadata.namespace
                scheduler_name = pod.spec.scheduler_name if hasattr(pod.spec, 'scheduler_name') else 'default-scheduler'
                
                # Only consider certain namespaces for placement metrics
                if pod_namespace in ['eo-workflow', 'data-locality-scheduler']:
                    node_type = node_info[node_name]['type'] if node_name in node_info else 'unknown'
                    region = node_info[node_name]['region'] if node_name in node_info else 'unknown'
                    zone = node_info[node_name]['zone'] if node_name in node_info else 'unknown'
                    
                    scheduler_pod_placement.labels(
                        scheduler_name=scheduler_name,
                        node_type=node_type,
                        region=region,
                        zone=zone,
                        pod_namespace=pod_namespace
                    ).inc()
                    
                    # Track counts for edge utilization calculations
                    total_pods['total'] += 1
                    total_pods[scheduler_name] += 1
                    
                    if node_type == 'edge':
                        edge_pods['total'] += 1
                        edge_pods[scheduler_name] += 1
                    else:
                        cloud_pods['total'] += 1
                        cloud_pods[scheduler_name] += 1
            
            # Calculate and record edge utilization
            for region in set(info['region'] for info in node_info.values()):
                for scheduler in ['data-locality-scheduler', 'default-scheduler']:
                    if total_pods[scheduler] > 0:
                        utilization = edge_pods[scheduler] / total_pods[scheduler]
                        edge_utilization.labels(
                            region=region,
                            zone='all',
                            scheduler_name=scheduler
                        ).set(utilization)
            
            # Simulate data transfer metrics between nodes
            # This will be random but representative data until we implement actual measurements
            for source in edge_nodes:
                for dest in edge_nodes:
                    if source != dest:
                        # Edge to edge: medium bandwidth, low latency
                        data_transfer_metrics.labels(source, dest, 'bandwidth_mbps').set(500)
                        data_transfer_metrics.labels(source, dest, 'latency_ms').set(2)
                
                for dest in cloud_nodes:
                    # Edge to cloud: lower bandwidth, higher latency
                    data_transfer_metrics.labels(source, dest, 'bandwidth_mbps').set(100)
                    data_transfer_metrics.labels(source, dest, 'latency_ms').set(20)
            
            for source in cloud_nodes:
                for dest in cloud_nodes:
                    if source != dest:
                        # Cloud to cloud: high bandwidth, low latency
                        data_transfer_metrics.labels(source, dest, 'bandwidth_mbps').set(1000)
                        data_transfer_metrics.labels(source, dest, 'latency_ms').set(1)
                
                for dest in edge_nodes:
                    # Cloud to edge: lower bandwidth, higher latency
                    data_transfer_metrics.labels(source, dest, 'bandwidth_mbps').set(100)
                    data_transfer_metrics.labels(source, dest, 'latency_ms').set(20)
                    
        except Exception as e:
            print(f"Error updating metrics: {e}")

    # Main loop
    while True:
        update_metrics()
        time.sleep(15)
