apiVersion: v1
kind: ConfigMap
metadata:
  name: data-locality-scheduler-config
  namespace: data-locality-scheduler

data:
  scheduler-config.yaml: |
    schedulerName: "data-locality-scheduler"
    podQueueSize: 100
    healthServerPort: 8080
    refreshInterval: "5m"
    enableMockData: false

    # filtering options
    percentageOfNodesToScore: 100
    minFeasibleNodesToFind: 100
    maxFailedAttempts: 5

    # caching options
    resourceCacheEnabled: true
    resourceCacheExpiration: "30s"
    storageCacheExpiration: "5m"

    # default weights
    resourceWeight: 0.15
    nodeAffinityWeight: 0.10
    nodeTypeWeight: 0.15
    capabilitiesWeight: 0.10
    dataLocalityWeight: 0.50

    # data-intensive weights
    dataIntensiveResourceWeight: 0.10
    dataIntensiveNodeAffinityWeight: 0.05
    dataIntensiveNodeTypeWeight: 0.10
    dataIntensiveCapabilitiesWeight: 0.05
    dataIntensiveDataLocalityWeight: 0.70

    # compute-intensive weights
    computeIntensiveResourceWeight: 0.40
    computeIntensiveNodeAffinityWeight: 0.15
    computeIntensiveNodeTypeWeight: 0.10
    computeIntensiveCapabilitiesWeight: 0.15
    computeIntensiveDataLocalityWeight: 0.20

    localBandwidth: 1000000000 # 1 GB/s
    sameZoneBandwidth: 600000000 # 600 MB/s
    sameRegionBandwidth: 200000000 # 200 MB/s
    crossRegionBandwidth: 50000000 # 50 MB/s
    edgeCloudBandwidth: 25000000 # 25 MB/s

    # latency settings in milliseconds
    localLatency: 0.1
    sameZoneLatency: 1.0
    sameRegionLatency: 5.0
    crossRegionLatency: 30.0
    edgeCloudLatency: 40.0

    # node scoring options
    minResourceScore: 0
    maxResourceScore: 100
    defaultScore: 50

    # debug options
    verboseLogging: true
    detailedMetrics: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-locality-scheduler
  namespace: data-locality-scheduler
  labels:
    app: data-locality-scheduler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: data-locality-scheduler
  template:
    metadata:
      labels:
        app: data-locality-scheduler
    spec:
      serviceAccountName: data-locality-scheduler
      containers:
      - name: scheduler
        image: davidandw190/data-locality-scheduler:latest
        imagePullPolicy: Always
        args:
        - "--config=/etc/scheduler/scheduler-config.yaml"
        - "--verbose=true"
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - containerPort: 8080
          name: http
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 20
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 300m
            memory: 512Mi
        volumeMounts:
        - name: config-volume
          mountPath: /etc/scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
      volumes:
      - name: config-volume
        configMap:
          name: data-locality-scheduler-config
---
apiVersion: v1
kind: Service
metadata:
  name: data-locality-scheduler
  namespace: data-locality-scheduler
  labels:
    app: data-locality-scheduler
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
    description: "Service exposing health check and API endpoints for the scheduler"
spec:
  selector:
    app: data-locality-scheduler
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  type: ClusterIP
