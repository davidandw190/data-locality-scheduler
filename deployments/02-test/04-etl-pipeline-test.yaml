apiVersion: v1
kind: ConfigMap
metadata:
  name: etl-pipeline-config
  namespace: data-locality-scheduler
data:
  pipeline.py: |
    import os
    import time
    import socket

    def run_pipeline_stage():
        stage = os.environ.get('PIPELINE_STAGE', 'unknown')
        print(f"Running {stage} stage of ETL pipeline")
        print(f"Node: {os.environ.get('NODE_NAME', 'unknown')}")
        time.sleep(3600)

    if __name__ == "__main__":
        run_pipeline_stage()
---
apiVersion: v1
kind: Pod
metadata:
  name: data-extract
  namespace: data-locality-scheduler
  labels:
    pipeline: etl
    pipeline-stage: extract
  annotations:
    data.scheduler.thesis/input-1: "raw-data/source-data.parquet,209715200,10,5,parquet"
    data.scheduler.thesis/output-1: "intermediate/extracted-data.json,104857600,0,5,json"
    scheduler.thesis/pipeline: "etl"
    scheduler.thesis/pipeline-stage: "1"
spec:
  schedulerName: data-locality-scheduler
  containers:
  - name: extract
    image: python:3.9
    command: [ "python", "/pipeline/pipeline.py" ]
    env:
    - name: PIPELINE_STAGE
      value: "extract"
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: pipeline-config
      mountPath: /pipeline
    resources:
      requests:
        memory: "256Mi"
        cpu: "200m"
      limits:
        memory: "512Mi"
        cpu: "400m"
  volumes:
  - name: pipeline-config
    configMap:
      name: etl-pipeline-config
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: data-transform
  namespace: data-locality-scheduler
  labels:
    pipeline: etl
    pipeline-stage: transform
  annotations:
    data.scheduler.thesis/input-1: "intermediate/extracted-data.json,104857600,15,5,json"
    data.scheduler.thesis/output-1: "intermediate/transformed-data.parquet,52428800,0,5,parquet"
    scheduler.thesis/pipeline: "etl"
    scheduler.thesis/pipeline-stage: "2"
spec:
  schedulerName: data-locality-scheduler
  containers:
  - name: transform
    image: python:3.9
    command: [ "python", "/pipeline/pipeline.py" ]
    env:
    - name: PIPELINE_STAGE
      value: "transform"
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: pipeline-config
      mountPath: /pipeline
    resources:
      requests:
        memory: "512Mi"
        cpu: "400m"
      limits:
        memory: "1Gi"
        cpu: "800m"
  volumes:
  - name: pipeline-config
    configMap:
      name: etl-pipeline-config
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: data-load
  namespace: data-locality-scheduler
  labels:
    pipeline: etl
    pipeline-stage: load
  annotations:
    data.scheduler.thesis/input-1: "intermediate/transformed-data.parquet,52428800,5,5,parquet"
    data.scheduler.thesis/output-1: "results/final-data.parquet,104857600,0,5,parquet"
    scheduler.thesis/pipeline: "etl"
    scheduler.thesis/pipeline-stage: "3"
    scheduler.thesis/prefer-cloud: "true"
spec:
  schedulerName: data-locality-scheduler
  containers:
  - name: load
    image: python:3.9
    command: [ "python", "/pipeline/pipeline.py" ]
    env:
    - name: PIPELINE_STAGE
      value: "load"
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: pipeline-config
      mountPath: /pipeline
    resources:
      requests:
        memory: "256Mi"
        cpu: "200m"
      limits:
        memory: "512Mi"
        cpu: "400m"
  volumes:
  - name: pipeline-config
    configMap:
      name: etl-pipeline-config
  restartPolicy: Never
