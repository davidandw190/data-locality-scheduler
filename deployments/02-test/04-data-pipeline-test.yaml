apiVersion: v1
kind: ConfigMap
metadata:
  name: data-pipeline-test
  namespace: data-locality-scheduler
data:
  pipeline.py: |
    import os
    import time
    import json
    import socket

    stage = os.environ.get("PIPELINE_STAGE", "unknown")
    node_name = os.environ.get("NODE_NAME", "unknown")

    print(f"Running pipeline stage '{stage}' on node: {node_name}")
    print(f"Hostname: {socket.gethostname()}")

    # Record execution details to shared volume
    with open(f"/data/{stage}-execution.json", "w") as f:
        json.dump({
            "stage": stage,
            "node": node_name,
            "timestamp": time.time()
        }, f)

    # Simulate processing workload
    print(f"Processing data for stage: {stage}")
    time.sleep(3600)
---
apiVersion: v1
kind: Pod
metadata:
  name: data-extract
  namespace: data-locality-scheduler
  annotations:
    data.scheduler.thesis/input-1: "source-data/raw-data.json,524288000,10,5,json"
    data.scheduler.thesis/output-1: "pipeline-data/extracted-data.json,104857600,0,5,json"
    scheduler.thesis/pipeline-stage: "extract"
    scheduler.thesis/data-intensive: "true"
spec:
  schedulerName: data-locality-scheduler
  containers:
  - name: extract-processor
    image: python:3.11
    command: [ "python", "/pipeline/pipeline.py" ]
    env:
    - name: PIPELINE_STAGE
      value: "extract"
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: pipeline-code
      mountPath: /pipeline
    - name: shared-data
      mountPath: /data
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"
  volumes:
  - name: pipeline-code
    configMap:
      name: data-pipeline-test
  - name: shared-data
    emptyDir: {}
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: data-transform
  namespace: data-locality-scheduler
  annotations:
    data.scheduler.thesis/input-1: "pipeline-data/extracted-data.json,104857600,15,5,json"
    data.scheduler.thesis/output-1: "pipeline-data/transformed-data.json,52428800,0,5,json"
    scheduler.thesis/pipeline-stage: "transform"
    scheduler.thesis/data-intensive: "true"
spec:
  schedulerName: data-locality-scheduler
  containers:
  - name: transform-processor
    image: python:3.11
    command: [ "python", "/pipeline/pipeline.py" ]
    env:
    - name: PIPELINE_STAGE
      value: "transform"
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: pipeline-code
      mountPath: /pipeline
    - name: shared-data
      mountPath: /data
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
  volumes:
  - name: pipeline-code
    configMap:
      name: data-pipeline-test
  - name: shared-data
    emptyDir: {}
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: data-load
  namespace: data-locality-scheduler
  annotations:
    data.scheduler.thesis/input-1: "pipeline-data/transformed-data.json,52428800,5,5,json"
    data.scheduler.thesis/output-1: "analytics/processed-data.json,26214400,0,5,json"
    scheduler.thesis/pipeline-stage: "load"
    scheduler.thesis/prefer-cloud: "true"
spec:
  schedulerName: data-locality-scheduler
  containers:
  - name: load-processor
    image: python:3.11
    command: [ "python", "/pipeline/pipeline.py" ]
    env:
    - name: PIPELINE_STAGE
      value: "load"
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: pipeline-code
      mountPath: /pipeline
    - name: shared-data
      mountPath: /data
    resources:
      requests:
        memory: "256Mi"
        cpu: "200m"
      limits:
        memory: "512Mi"
        cpu: "400m"
  volumes:
  - name: pipeline-code
    configMap:
      name: data-pipeline-test
  - name: shared-data
    emptyDir: {}
  restartPolicy: Never
